services:
  migrate:
    image: ${API_IMAGE}
    command: migrate
    environment:
      PORT: 3000
      ENVIRONMENT: staging
      GIT_BRANCH: ${GIT_BRANCH:-unknown}
      DEPLOYMENT_ENVIRONMENT: ${DEPLOYMENT_ENVIRONMENT:-staging}
      # Production database URL for data migration
      PRODUCTION_DATABASE_URL: ${PRODUCTION_DATABASE_URL}
      # Test user credentials for staging setup
      TEST_SUPERADMIN_EMAIL: ${TEST_SUPERADMIN_EMAIL}
      TEST_ADMIN_EMAIL: ${TEST_ADMIN_EMAIL}
      TEST_USER_EMAIL: ${TEST_USER_EMAIL}
      TEST_SUPERADMIN_PASSWORD: ${TEST_SUPERADMIN_PASSWORD}
      TEST_ADMIN_PASSWORD: ${TEST_ADMIN_PASSWORD}
      TEST_USER_PASSWORD: ${TEST_USER_PASSWORD}
    env_file:
      - staging.env
    networks:
      - backend
    deploy:
      replicas: 1
      placement:
        constraints: [node.role == manager]
      restart_policy:
        # Use 'none' for one-shot migration tasks - don't restart after completion
        condition: none
      # Ensure only one migrate service runs at a time to prevent migration conflicts
      update_config:
        parallelism: 1
        delay: 10s
        # Don't rollback if migration completes (exits 0) - that's expected behavior
        failure_action: pause
        # Short monitor time since migration should complete quickly
        monitor: 10s
        order: stop-first

  api:
    image: ${API_IMAGE}
    command: start
    # Enable EC2 instance role credentials for AWS SDK (S3, etc.)
    extra_hosts:
      - "169.254.169.254:169.254.169.254"
    environment:
      PORT: 3000
      ENVIRONMENT: staging
      GIT_COMMIT_SHA: ${GIT_COMMIT_SHA:-unknown}
      GIT_BRANCH: ${GIT_BRANCH:-unknown}
      DEPLOYMENT_ENVIRONMENT: ${DEPLOYMENT_ENVIRONMENT:-staging}
    ports:
      - target: 3000
        published: 3002
        protocol: tcp
        mode: ingress
    env_file:
      - staging.env
    networks:
      - backend
      - execution
    deploy:
      replicas: 1
      resources:
        reservations:
          cpus: "0.1"
          memory: 100M
      # Rolling update configuration to prevent race conditions
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
        order: start-first
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
        order: stop-first
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  worker:
    image: ${API_IMAGE}
    command: worker
    # Enable EC2 instance role credentials for AWS SDK (S3, etc.)
    extra_hosts:
      - "169.254.169.254:169.254.169.254"
    environment:
      PORT: 3000
      ENVIRONMENT: staging
      # Only handle execution tasks, not build tasks
      CELERY_WORKER_QUEUES: default
      GIT_BRANCH: ${GIT_BRANCH:-unknown}
      DEPLOYMENT_ENVIRONMENT: ${DEPLOYMENT_ENVIRONMENT:-staging}
    env_file:
      - staging.env
    networks:
      - backend
    deploy:
      replicas: 1
      resources:
        reservations:
          cpus: "0.1"
          memory: 100M
      # Rolling update configuration to prevent race conditions
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  beat:
    image: ${API_IMAGE}
    command: beat
    # Enable EC2 instance role credentials for AWS SDK (S3, etc.)
    extra_hosts:
      - "169.254.169.254:169.254.169.254"
    environment:
      PORT: 3000
      ENVIRONMENT: staging
      GIT_BRANCH: ${GIT_BRANCH:-unknown}
      DEPLOYMENT_ENVIRONMENT: ${DEPLOYMENT_ENVIRONMENT:-staging}
    env_file:
      - staging.env
    networks:
      - backend
    deploy:
      replicas: 1
      resources:
        reservations:
          cpus: "0.1"
          memory: 100M
      # Rolling update configuration to prevent race conditions
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  docker:
    image: ${API_IMAGE}
    command: worker
    user: root
    # Enable EC2 instance role credentials for AWS SDK (S3, ECR, etc.)
    extra_hosts:
      - "169.254.169.254:169.254.169.254"
    environment:
      PORT: 3000
      ENVIRONMENT: staging
      # Only handle build tasks that require Docker socket access
      CELERY_WORKER_QUEUES: build
      GIT_BRANCH: ${GIT_BRANCH:-unknown}
      DEPLOYMENT_ENVIRONMENT: ${DEPLOYMENT_ENVIRONMENT:-staging}
    env_file:
      - staging.env
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    networks:
      - backend
    deploy:
      placement:
        constraints: [node.role == manager]
      replicas: 1
      resources:
        reservations:
          cpus: "0.5"
          memory: 800M
        limits:
          cpus: "1.0"
          memory: 1G
      # Rolling update configuration for builder
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.5
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  # Global Docker cleanup service - runs on ALL nodes (managers + workers)
  # This ensures images, containers, and build cache are cleaned on every node,
  # not just managers where the Celery cleanup tasks run.
  # Security note: Uses minimal docker:24-cli image with only prune commands.
  docker-cleanup:
    image: docker:24-cli
    user: root
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    # Run cleanup every 6 hours: prune containers, dangling images, and build cache
    command: |
      sh -c 'while true; do
        echo "[$(date)] Starting Docker cleanup on node..."
        echo "Pruning stopped containers..."
        docker container prune -f
        echo "Pruning dangling images..."
        docker image prune -f
        echo "Pruning build cache (older than 168h)..."
        docker builder prune -f --filter "until=168h" || true
        echo "Pruning unused volumes..."
        docker volume prune -f || true
        echo "[$(date)] Cleanup complete. Sleeping 6 hours..."
        sleep 21600
      done'
    deploy:
      mode: global  # Run on EVERY node in the swarm
      resources:
        reservations:
          cpus: "0.1"
          memory: 50M
        limits:
          cpus: "0.5"
          memory: 100M
      restart_policy:
        condition: on-failure
        delay: 60s
        max_attempts: 3
        window: 300s

  postgres:
    image: postgis/postgis:16-3.4
    environment:
      POSTGRES_DB: ${STAGING_DB_NAME:-trendsearth_staging}
      POSTGRES_USER: ${STAGING_DB_USER:-trendsearth_staging}
      POSTGRES_PASSWORD: ${STAGING_DB_PASSWORD:-postgres}
    ports:
      - target: 5432
        published: 5433
        protocol: tcp
        mode: ingress
    volumes:
      - postgres_staging_data:/var/lib/postgresql/data
    configs:
      - source: postgres_init
        target: /docker-entrypoint-initdb.d/init-postgis.sh
        mode: 0755
    networks:
      - backend
    deploy:
      placement:
        constraints:
          - node.role == manager
      resources:
        reservations:
          cpus: "0.1"
          memory: 200M
      # Rolling update configuration for database
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 120s

  redis:
    image: redis:7-alpine
    ports:
      - target: 6379
    volumes:
      # Persistent Redis data storage - retains data across deployments
      - redis_staging_data:/data
    networks:
      - backend
    deploy:
      placement:
        constraints: [node.role == manager]
      resources:
        reservations:
          cpus: "0.1"
          memory: 100M
      # Rolling update configuration for Redis
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
        monitor: 60s
        max_failure_ratio: 0.3
      rollback_config:
        parallelism: 1
        delay: 0s
        failure_action: pause
        monitor: 60s
    # Append-only file persistence for Redis data durability
    command: redis-server --appendonly yes --appendfsync everysec

volumes:
  postgres_staging_data:
  # Persistent Redis data volume - maintains session data, cache, and Celery task state across deployments
  redis_staging_data:
    driver: local

networks:
  backend:
    driver: overlay
    attachable: true
    ipam:
      driver: default
      config:
        # Use 10.20.x.x for staging (prod uses 10.10.x.x)
        - subnet: ${DOCKER_SUBNET:-10.20.0.0/16}
  execution:
    driver: overlay
    attachable: true
    ipam:
      driver: default
      config:
        # Use 10.21.x.x for staging execution (separate from backend)
        - subnet: ${EXECUTION_SUBNET:-10.21.0.0/24}

configs:
  postgres_init:
    file: ./config/db/init-postgis.sh
